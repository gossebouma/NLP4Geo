{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "{",
          "\"tags\":",
          "[",
          "\"full-width\"",
          "]",
          "}"
        ],
        "id": "XHEEQ2RJMl6H"
      },
      "source": [
        "## Introduction to NLP with spaCy\n",
        "\n",
        "spaCy is a Python library for Natural Language Processing. It is robust and versatile, here we demonstrate how to install and use it.\n",
        "\n",
        "This notebook illustrates how to load a language model, and how to analyse text with it.\n",
        "\n",
        "See also spacy 101, https://spacy.io/usage/spacy-101\n",
        "\n",
        "This notebook is part of a hands-on introduction to NLP for Geo-text data mining.\n",
        "\n",
        "### Topics covered in this notebook\n",
        "\n",
        "1. Loading a spaCy model \n",
        "2. Selecting words by their part-of-speech \n",
        "3. Visualising parse results\n",
        "4. pattern matching and information extraction\n",
        "5. Semantic similarity (of words, of sentences)\n",
        "\n",
        "\n",
        "### Other topics (check the other notebooks or instruction pages)\n",
        "\n",
        "6. Named Entities (visualisation, explain labels) (notebook)\n",
        "7. Entity Linking (to geonames) (notebook)\n",
        "8. Information Extraction with SPIKE (demo, examples)\n",
        "9. (optional) Information Extraction with Spacy\n",
        "10. LLMs and ChatGPT for Information Extraction\n",
        "\n",
        "### 1. Loading Spacy\n",
        "\n",
        "To load the spacy library, you first need to [install](https://spacy.io/usage) it (unless you are running this tutorial on Google colab). In addition, you need to select a **model** for the language that you want to analyze. Models generally come in 3 flavors, from small to large. Smaller models load and run faster, larger models are more accurate. For English there is also the transformer model, the most accurate model for many tasks and datasets. \n",
        "\n",
        "spacy.info() gives information about your installation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jEKV3ZMMl6Y"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# 1. loading the model may produce some errors from tensorflow. Do not worry about these \n",
        "# 2. Google colab only has en_core_web_sm pre-installed, run command below to install other models \n",
        "# !python -m spacy download en_core_web_lg\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "spacy.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZfA0JorMl6g"
      },
      "source": [
        "### Visualisation\n",
        "\n",
        "The easiest way to understand the output of analysis is to visualise the result. This works best for short input strings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IVPsNDEMl6m"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(\"New York is a large city\")\n",
        "displacy.render(doc, style=\"dep\",minify=True,jupyter=True,options={'distance':130})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpYUPvUiMl6r"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "* Change the example above with a sentence that contains your name in it, and display the result. \n",
        "* Does your example contain words that are labeled with the part-of-speech tag PROPN? What does this tell you?\n",
        "* Proper names often consist of more than 1 word (_New York_). Is there a syntactic relation that tells you when two words that form a single proper name are indeed recognized as one name by the model?\n",
        "\n",
        "\n",
        "Apart from visualising the output of the model, we can also process it. The result of analyzing a text with Spacy is a list of *token* objects that have attributes such as .pos_ (note the underscore!) for its part-of-speech text. By iterating over the list of tokens, we can select certain tokens, and print some of its properties, as illustrated below. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc9ALOTGMl6v"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "Chatbots like ChatGPT are used by hundreds of millions of people for an increasingly wide array of tasks, including email services, online tutors and search engines. \n",
        "And they could change the way people interact with information. \n",
        "But there is no way of ensuring that these systems produce information that is accurate.\n",
        "'''\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for word in doc:\n",
        "    if word.pos_ == \"NOUN\" :\n",
        "        print(word.text, word.lemma_, word.pos_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc9fNmtYMl6z"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "* Replace the text above with a recent news item. Modify the code to list all the proper names in the text. \n",
        "\n",
        "Side note: Long texts can also be visualised by Spacy on a per sentence basis. Try it for your example text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "full-width"
        ],
        "id": "KdV7g-seMl62"
      },
      "outputs": [],
      "source": [
        "sentence_spans = list(doc.sents)\n",
        "displacy.render(sentence_spans, style=\"dep\",minify=True,jupyter=True,options={'distance':100})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1_iq_7_Ml67"
      },
      "source": [
        "### Selecting combinations fo words\n",
        "\n",
        "Sometimes we want to find concepts in a text, like names of places or organisations, or for instance, the most important keywords and concepts in a text (i.e. for visualisation in a word cloud). Such concepts are not always a single word, but often consist of two or more words. The **phrase matcher** allows you to identify such word combinations using all the properties that spacy has assigned to the tokens in the input, and using regular expression operators for creating flexible patterns.\n",
        "\n",
        "The example below, finds combinations consisting of an ADJECTIVE and a NOUN ('artificial intelligence').\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5v74mhnMl6-"
      },
      "outputs": [],
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "\n",
        "# https://spacy.io/api/matcher#add as of version 3, longest match is supported\n",
        "matcher.add(\"adj_noun\", [pattern],greedy=\"LONGEST\")\n",
        "\n",
        "text = '''Some old green trees have thick stems. \n",
        "Artificial intelligence is a booming business. \n",
        "NVIDIA is a computer firm that is at the forefront.'''\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "# the name of your pattern is hashed. To map from hash-id to text, look it up in vocab.strings\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(nlp.vocab.strings[match_id], \": \", span.text)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp5l4RvGMl7B"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "* Modify the adj_noun pattern, so that the pattern also matches nouns preceded by zero or more adjectives \n",
        "\n",
        "* Modify it, so that the pattern also matches with noun noun compounds like _'computer firm'_\n",
        "\n",
        "See [operators](https://spacy.io/usage/rule-based-matching#quantifiers) for examples of adding regular expression operators to your patterns (for matching optionally, or zero or more times)\n",
        "\n",
        "Test your pattern on an input text that contains examples of such cases. Does it always work as expected?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI1avKvoMl7C"
      },
      "source": [
        "### Optional: Towards Information Extraction\n",
        "\n",
        "The pattern matcher can also to some extent be used for extracting words by means of their syntactic relationships in text. This can be useful for information extraction. Imagine for instance that you want to extract all occurrences of organisations in a text, and, if they are the object of verb, also the verb (i.e. from the sentence _Google wants to buy NVIDIA_, it would extract the tuple _(buy, NVIDIA)_.\n",
        "\n",
        "Tokens contain information about their part-of-speech etc., but also syntactic information, such as their syntactic function. These are the labels on the edges between nodes you see in the displacy visualisations. 'nsubj' is the dependency label for subjects, 'dobj' is the dependency label for objects.\n",
        "\n",
        "We can use this information to also find the relationship between a matched span and the word of which it is a dependent. The example below matches proper names (sequences with PROPN as part-of-speech, we will soon see better ways to find named entities) and prints their dependency label (dep_) as well as the word of wich they are a dependent. \n",
        "\n",
        "One twist here is that the span (i.e. a sequence of tokens) that matches a pattern does not have these linguistic attributes. The attribute root is used to find the most prominent word within a span. This root token then provides information such as the dependency label and a link to the head.\n",
        "                                                                                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dfjRbETMl7E"
      },
      "outputs": [],
      "source": [
        "object_pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}]\n",
        "\n",
        "matcher.add(\"OBJECT\", [object_pattern])\n",
        "\n",
        "doc = nlp(\"Google wants to buy Open AI\")\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "    span = doc[start:end]\n",
        "    print(span.text, span.root.dep_,span.root.head.text)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J33d7jefMl7G"
      },
      "source": [
        "### Exercise 5 (optional)\n",
        "\n",
        "* Modify the code above so that it only prints cases where the root of the matched span is a direct object (dobj).\n",
        "* Modify the code above so that it prints the lemma_ of the head token (the verb) instead of the text. Test this for an example like 'Google buys Open AI'.\n",
        "\n",
        "Hint: these modifications require modifying the for-loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PToD4pDIMl7H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}