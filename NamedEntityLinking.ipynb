{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTwPrmyCSDa-"
   },
   "source": [
    "# Named Entity Classification\n",
    "\n",
    "A crucial part of any geocoding task is identification of named entities in the input text and classifying them as person, geo-location, organisation, etc. Here we demonstrate how spaCy handles this task using the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5946,
     "status": "ok",
     "timestamp": 1683867270797,
     "user": {
      "displayName": "G. Bouma",
      "userId": "12726716092195580447"
     },
     "user_tz": -120
    },
    "id": "sR38NJVgSDbE"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# 1. loading the model may produce some errors from tensorflow. Do not worry about these \n",
    "# 2. Google colab only has en_core_web_sm pre-installed, run command below to install other models \n",
    "# !python -m spacy download en_core_web_trf\n",
    "\n",
    "import spacy_transformers\n",
    "nlp = spacy.load('en_core_web_trf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE5v9s9ASDbG"
   },
   "source": [
    "### Spacy Named Entity Recognition\n",
    "\n",
    "First, we test the NER component, i.e. which entities are recognized by a standard Spacy model (lg/trf)?\n",
    "\n",
    "The lg model does a reasonable job at recognizing NEs although a few are missed as well. The easiest way to improve coverage is to add manual patterns, as explained below. Another option is to try the transformer model (trf), which in many ways is more accurate than the lg model.\n",
    "\n",
    "For NER, trf seems to be the preferred model, but remember that installation can be a bit tricky (requires proper installation of the spacy transformer lib)\n",
    "\n",
    "We can also visualise results using the displacy library.\n",
    "\n",
    "Wonder what all the labels mean? Try spacy.explain('LABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1563,
     "status": "ok",
     "timestamp": 1683867285088,
     "user": {
      "displayName": "G. Bouma",
      "userId": "12726716092195580447"
     },
     "user_tz": -120
    },
    "id": "wpx5h28NSDbI",
    "outputId": "ba4cb6b3-200d-4705-9756-d6a943bbe5bb"
   },
   "outputs": [],
   "source": [
    "# text = '''\n",
    "# Prime minister Rutte and vice minister Vijlbrief visited Garmerwolde yesterday. \n",
    "# They came from The Hague to Groningen to talk about compensation for the damage done by drilling in Loppersum.\n",
    "# '''\n",
    "\n",
    "text = '''At the UG’s Campus Fryslân faculty in Leeuwarden, Van Vulpen is conducting research into discontent in different regions of the Netherlands. The results of the recent provincial elections, in which the new Farmer-Citizen Movement (BBB) scored a thumping victory over the established parties, suggest that the government is the main object of that discontent. Van Vulpen: ‘The BBB used public disaffection to its advantage, profiling itself as a rural party and magnifying the differences between city and countryside.’\n",
    "Much of the support for the BBB came from the fringe regions outside the Randstad. In parts of the province of Overijssel, the BBB raked in almost 60% of the votes. Van Vulpen’s goal is to uncover the reasons underlying this regional discontent with the government and the established parties. Because of his expertise in this area, his opinion is often sought by the Dutch media.'''\n",
    "\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents :\n",
    "    print(sent.text)\n",
    "    for ent in sent.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "        \n",
    "spacy.explain('FAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1683867297691,
     "user": {
      "displayName": "G. Bouma",
      "userId": "12726716092195580447"
     },
     "user_tz": -120
    },
    "id": "eDPcAzZpSDbK",
    "outputId": "cc711eed-14f7-41ab-8328-d478bbb17dce"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy \n",
    "\n",
    "# displacy.serve(doc, style=\"ent\")\n",
    "# inside a jupyter notebook use this: \n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTQuCiQiSDbL"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "* What is the label used for geographical entities? (use spacy.explain(LABEL) to see an explanation for the labels)\n",
    "* Replace the **text** above with an alternative text containing names for geographical locations, and check whether spaCy correctly identifies these (i.e. whether the string is correct, and whether the class label is correct).\n",
    "  *  Are any non-geograhical entities labeled as geographical?\n",
    "  * Are all geo locations labeled as geograhical?\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1683868184271,
     "user": {
      "displayName": "G. Bouma",
      "userId": "12726716092195580447"
     },
     "user_tz": -120
    },
    "id": "g07h5TyMXC1O",
    "outputId": "d2d9fed6-8dbd-4619-9030-f387b85e7c64"
   },
   "outputs": [],
   "source": [
    "## You can use ent.label_ to filter named entities by their label/category\n",
    "\n",
    "for ent in doc.ents : \n",
    "  print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zim5wZE_XeCg"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Modify the code above so that it only prints geographical named entities\n",
    "\n",
    "## Entity linking with the wikidata entity finder api\n",
    "\n",
    "Geocoding is the task where geographical names in a text are linked to an actual location. Entity linking is the same task, but more general. Entity linking provides an unambiguous ID (such as a wikipedia page) for names in a text. \n",
    "\n",
    "For entity linking, you can use the __wikidata api__ for finding the corresponding entity IDs. ((Wikidata)[https://www.wikidata.org/] is like a database with facts from Wikipedia.) Note that the api can give multiple results for a given text string. Here, we simply return the id of the first result. Apart from id, the api also returns a label and description for each match, that could perhaps be used for disambiguation. (Ie prefer entities with certain keywords in the description field, like *actor, film, ....*)\n",
    "\n",
    "Integration with Spacy is done by introducing an extension attribute on the spans found by NER. See https://spacy.io/usage/rule-based-matching#entityruler for an example and discussion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1683868982803,
     "user": {
      "displayName": "G. Bouma",
      "userId": "12726716092195580447"
     },
     "user_tz": -120
    },
    "id": "vn-A3rjNSDbM"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from spacy.tokens import Span\n",
    "    \n",
    "def wikidata_entity_link(span) :\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbsearchentities', \n",
    "              'language':'en',\n",
    "              'format':'json',\n",
    "              'search': span.text}\n",
    "    json = requests.get(url,params).json()\n",
    "    # this part can be replaced by fancier disambiguation methods, \n",
    "    # or returning a list of ids from all search results\n",
    "    try : \n",
    "        wd_id = json['search'][0]['id']\n",
    "        try :\n",
    "            wd_desc = json['search'][0]['description']\n",
    "        except KeyError :\n",
    "            wd_desc = ''\n",
    "    except IndexError :\n",
    "        (wd_id, wd_desc) = ('no_id_found','')\n",
    "    return (wd_id,wd_desc)\n",
    "            \n",
    "Span.set_extension('wikidata_id',getter=wikidata_entity_link,force=True)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Execute the code above to include the wikidata entity linker in the saCy entity pipeline. Now analyse the example text again and see what it provides as unambiguous ID's for the names in the text. \n",
    "\n",
    "* You will notice that not all results are correct. \n",
    "* Try some other texts containing names and see whether this bevaviour is also true for other texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3816,
     "status": "ok",
     "timestamp": 1683868991407,
     "user": {
      "displayName": "G. Bouma",
      "userId": "12726716092195580447"
     },
     "user_tz": -120
    },
    "id": "ijQ2CgNpaURD",
    "outputId": "7daf8515-4a81-44c4-88ce-27acad87122f"
   },
   "outputs": [],
   "source": [
    "# uuncomment and modify the lines below to try other text\n",
    "\n",
    "#text = ''' “Although the final results are not in yet, we are leading by far,” \n",
    "# Mr. Erdogan told supporters gathered outside his party’s headquarters in Ankara, the capital.\n",
    "# '''\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "        print(ent.text, ent._.wikidata_id)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_i0n7uDSDbN"
   },
   "source": [
    "### Exercise 4 (optional)\n",
    "\n",
    "The Wikidata entity linker uses the text of a named entity recognized by spaCy to look up entities in wikidata, and simply returns the first hit. For BBB for instance, this gives a wrong answer, even though the correct entity is present in wikidata as well.\n",
    "\n",
    "* Can you think of a method to improve the performance of our simple entity linker?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
